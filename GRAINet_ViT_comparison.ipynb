{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRAINet vs ViT-Tiny Comparison: Grain Size Distribution Prediction\n",
    "\n",
    "This notebook compares the original ResNet-FCN architecture with the new ViT-Tiny implementation using STRING2D-Cayley positional encoding for grain size distribution prediction.\n",
    "\n",
    "## Key Innovations\n",
    "- **ViT-Tiny**: Lightweight transformer with ~5.8M parameters\n",
    "- **STRING2D-Cayley Encoding**: Advanced positional encoding using antisymmetric matrix\n",
    "- **Adaptive Image Processing**: Automatic scaling and center cropping to 224×224\n",
    "- **JAX/Flax NNX**: Modern neural network framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.nnx as nnx\n",
    "import optax\n",
    "import os\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import time\n",
    "\n",
    "# Import our ViT implementation\n",
    "from vit_flax_nnx import create_vit_model, center_crop, print_vit_flax_architecture\n",
    "\n",
    "# Original GRAINet imports\n",
    "from keras.layers import Input\n",
    "from resnet_architecture import FCN_grainsize\n",
    "import preprocessing as prepro\n",
    "from helper import setup_parser, collect_cv_data, create_k_fold_split_indices\n",
    "from train_test import run_train, run_test\n",
    "from test_vis import create_plots\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Arguments and Data Paths\n",
    "\n",
    "Following the same data loading pattern from the original GRAINet notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup argument parser with default values (from original notebook)\n",
    "parser = setup_parser()\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "# Image dataset with ground truth \n",
    "args.data_npz_path = os.path.join('data_GRAINet_demo', 'data_KLEmme_1bank.npz')\n",
    "\n",
    "# Full orthophoto image (for predicting a map)\n",
    "args.image_path = os.path.join('data_GRAINet_demo', 'orthophoto_KLEmme.tif')\n",
    "\n",
    "# Manually created mask to select regions of interest on the gravel bar\n",
    "ortho_mask_path = os.path.join('data_GRAINet_demo', 'mask_dm_pred.tif')\n",
    "\n",
    "# Set output directories for both models\n",
    "parent_dir_resnet = 'output_demo_dm_resnet'\n",
    "parent_dir_vit = 'output_demo_dm_vit'\n",
    "\n",
    "# Evaluation metrics\n",
    "metrics_keys = ('mae', 'rmse')\n",
    "\n",
    "# Training parameters\n",
    "args.verbose = 0  # minimal output\n",
    "args.nb_epoch = 20\n",
    "\n",
    "# Create output directories\n",
    "for parent_dir in [parent_dir_resnet, parent_dir_vit]:\n",
    "    if not os.path.exists(parent_dir):\n",
    "        os.makedirs(parent_dir)\n",
    "\n",
    "# Print all arguments\n",
    "print(\"Experiment Configuration:\")\n",
    "for arg in vars(args):\n",
    "    print(f'{arg}: {getattr(args, arg)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Dataset\n",
    "\n",
    "Using the same data loading and cross-validation split approach as the original notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds = 10\n",
    "\n",
    "# Load dataset\n",
    "data = np.load(args.data_npz_path, allow_pickle=True)\n",
    "print('Data keys:', list(data.keys()))\n",
    "print('Image shape:', data['images'].shape)\n",
    "print('Labels shape:', data['labels'].shape)\n",
    "\n",
    "# Set output paths to save indices\n",
    "args.randCV_indices_path = os.path.join(parent_dir_resnet, f'random_{num_folds}_fold_indices.npy')\n",
    "\n",
    "# Create the non-overlapping data splits\n",
    "indices_list = create_k_fold_split_indices(data=data, out_path=args.randCV_indices_path, num_folds=num_folds)\n",
    "print(f'Created {num_folds} cross-validation folds')\n",
    "print(f'First fold size: {len(indices_list[0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture Comparison\n",
    "\n",
    "Initialize both ResNet-FCN and ViT-Tiny models for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print ViT architecture overview\n",
    "print_vit_flax_architecture()\n",
    "\n",
    "# Create models for comparison\n",
    "original_input_shape = (500, 200, 3)  # Original GRAINet input size\n",
    "\n",
    "# ResNet-FCN (Original GRAINet)\n",
    "img_input_original = Input(shape=original_input_shape)\n",
    "resnet_model = FCN_grainsize(img_input_original, bins=21, output_scalar=True)\n",
    "\n",
    "# ViT-Tiny with STRING2D-Cayley (Flax NNX) - with adaptive preprocessing\n",
    "rngs = nnx.Rngs(42)\n",
    "vit_flax_model = create_vit_model(image_size=224, bins=21, output_scalar=True, rngs=rngs)\n",
    "\n",
    "# Count parameters\n",
    "def count_keras_parameters(model):\n",
    "    return sum([np.prod(p.shape) for p in model.trainable_weights])\n",
    "\n",
    "def count_flax_parameters(model):\n",
    "    return sum([np.prod(p.shape) for p in jax.tree_leaves(nnx.state(model, nnx.Param))])\n",
    "\n",
    "resnet_params = count_keras_parameters(resnet_model)\n",
    "vit_flax_params = count_flax_parameters(vit_flax_model)\n",
    "\n",
    "print('\\nModel Comparison:')\n",
    "print(f'ResNet-FCN parameters: {resnet_params:,}')\n",
    "print(f'ViT-Tiny (Flax NNX) parameters: {vit_flax_params:,}')\n",
    "print(f'Parameter ratio (ViT/ResNet): {vit_flax_params/resnet_params:.2f}')\n",
    "\n",
    "# Test adaptive scaling and cropping functionality\n",
    "print('\\n=== Adaptive Scaling + Center Cropping Test ===')\n",
    "\n",
    "test_cases = [\n",
    "    (\"Small image (100×100)\", jnp.ones((1, 100, 100, 3))),\n",
    "    (\"Small rectangle (150×100)\", jnp.ones((1, 150, 100, 3))),\n",
    "    (\"GRAINet size (500×200)\", jnp.ones((1, 500, 200, 3))),\n",
    "    (\"Medium (300×400)\", jnp.ones((1, 300, 400, 3))),\n",
    "    (\"Perfect size (224×224)\", jnp.ones((1, 224, 224, 3)))\n",
    "]\n",
    "\n",
    "for description, dummy_input in test_cases:\n",
    "    original_shape = dummy_input.shape[1:3]\n",
    "    flax_output = vit_flax_model(dummy_input)\n",
    "    \n",
    "    min_dim = min(original_shape)\n",
    "    scale_factor = 224 / min_dim if min_dim < 224 else 1.0\n",
    "    action = \"Scale up\" if scale_factor > 1.0 else \"Crop only\"\n",
    "    \n",
    "    print(f'{description}: {original_shape[0]}×{original_shape[1]} → 224×224')\n",
    "    print(f'  Action: {action} (scale: {scale_factor:.2f}x)')\n",
    "    print(f'  Output shape: {flax_output.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet-FCN Training (Original GRAINet)\n",
    "\n",
    "Train the original ResNet-FCN model using the established GRAINet training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_runs = 1  # To evaluate over all samples use: N_runs = num_folds \n",
    "\n",
    "print('=== Training ResNet-FCN (Original GRAINet) ===')\n",
    "for test_fold_index in range(N_runs):\n",
    "    args.test_fold_index = test_fold_index\n",
    "    \n",
    "    args.experiment_dir = os.path.join(parent_dir_resnet, 'loss_{}'.format(args.loss_key), 'testfold_{}'.format(args.test_fold_index))\n",
    "    print('******************')\n",
    "    print('TEST FOLD: ', args.test_fold_index)\n",
    "    print(args.experiment_dir)\n",
    "\n",
    "    # Train the CNN\n",
    "    print('Training ResNet-FCN...')\n",
    "    run_train(args)\n",
    "    \n",
    "    # Test the best solution on the test data\n",
    "    print('Testing ResNet-FCN...')\n",
    "    run_test(args)\n",
    "    create_plots(args)\n",
    "\n",
    "print('ResNet-FCN training completed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ViT-Tiny Training with JAX/Flax NNX\n",
    "\n",
    "Implement custom training loop for ViT-Tiny with STRING2D-Cayley encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vit_training_data(data, indices_list, test_fold_index):\n",
    "    \"\"\"Prepare training and test data for ViT model\"\"\"\n",
    "    # Get train and test indices\n",
    "    test_indices = indices_list[test_fold_index]\n",
    "    train_indices = []\n",
    "    for i, fold_indices in enumerate(indices_list):\n",
    "        if i != test_fold_index:\n",
    "            train_indices.extend(fold_indices)\n",
    "    \n",
    "    # Split data\n",
    "    X_train = data['images'][train_indices]\n",
    "    y_train = data['dm'][train_indices]  # Using dm (mean diameter) for scalar regression\n",
    "    X_test = data['images'][test_indices]\n",
    "    y_test = data['dm'][test_indices]\n",
    "    \n",
    "    # Convert to JAX arrays\n",
    "    X_train = jnp.array(X_train, dtype=jnp.float32) / 255.0  # Normalize to [0,1]\n",
    "    y_train = jnp.array(y_train, dtype=jnp.float32)\n",
    "    X_test = jnp.array(X_test, dtype=jnp.float32) / 255.0\n",
    "    y_test = jnp.array(y_test, dtype=jnp.float32)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def train_vit_model(model, X_train, y_train, X_test, y_test, epochs=20, batch_size=4, learning_rate=1e-4):\n",
    "    \"\"\"Train ViT model using JAX/Flax NNX\"\"\"\n",
    "    # Create optimizer\n",
    "    optimizer = nnx.Optimizer(model, optax.adamw(learning_rate))\n",
    "    \n",
    "    # Training metrics\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        epoch_losses = []\n",
    "        \n",
    "        # Shuffle training data\n",
    "        n_samples = len(X_train)\n",
    "        indices = jax.random.permutation(jax.random.PRNGKey(epoch), n_samples)\n",
    "        X_train_shuffled = X_train[indices]\n",
    "        y_train_shuffled = y_train[indices]\n",
    "        \n",
    "        # Mini-batch training\n",
    "        for i in range(0, n_samples, batch_size):\n",
    "            batch_X = X_train_shuffled[i:i+batch_size]\n",
    "            batch_y = y_train_shuffled[i:i+batch_size]\n",
    "            \n",
    "            # Define loss function\n",
    "            def loss_fn(model):\n",
    "                predictions = model(batch_X, training=True)\n",
    "                loss = jnp.mean((predictions.squeeze() - batch_y) ** 2)\n",
    "                return loss\n",
    "            \n",
    "            # Compute loss and gradients\n",
    "            loss, grads = nnx.value_and_grad(loss_fn)(model)\n",
    "            \n",
    "            # Update model\n",
    "            optimizer.update(grads)\n",
    "            \n",
    "            epoch_losses.append(loss)\n",
    "        \n",
    "        # Compute epoch metrics\n",
    "        train_loss = jnp.mean(jnp.array(epoch_losses))\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Test loss\n",
    "        test_predictions = model(X_test, training=False)\n",
    "        test_loss = jnp.mean((test_predictions.squeeze() - y_test) ** 2)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            print(f'Epoch {epoch:3d}: Train Loss = {train_loss:.4f}, Test Loss = {test_loss:.4f}')\n",
    "    \n",
    "    return train_losses, test_losses\n",
    "\n",
    "print('=== Training ViT-Tiny with STRING2D-Cayley ===')\n",
    "test_fold_index = 0\n",
    "\n",
    "# Prepare training data\n",
    "X_train, y_train, X_test, y_test = create_vit_training_data(data, indices_list, test_fold_index)\n",
    "\n",
    "print(f'Training set: {X_train.shape}, Test set: {X_test.shape}')\n",
    "print(f'Training labels: {y_train.shape}, Test labels: {y_test.shape}')\n",
    "\n",
    "# Train ViT model\n",
    "start_time = time.time()\n",
    "train_losses, test_losses = train_vit_model(vit_flax_model, X_train, y_train, X_test, y_test, epochs=args.nb_epoch)\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f'ViT-Tiny training completed in {training_time:.2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect and Compare Results\n",
    "\n",
    "Evaluate both models and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect ResNet-FCN results using original GRAINet evaluation\n",
    "_, _, dm_results_dict = collect_cv_data(parent_dir=parent_dir_resnet, loss_keys=(args.loss_key,))\n",
    "\n",
    "# Get ViT-Tiny predictions\n",
    "vit_predictions = vit_flax_model(X_test, training=False).squeeze()\n",
    "\n",
    "# Calculate metrics\n",
    "resnet_mae = np.mean(np.abs(dm_results_dict[args.loss_key]['dm_true'] - dm_results_dict[args.loss_key]['dm_pred']))\n",
    "vit_mae = np.mean(np.abs(y_test - vit_predictions))\n",
    "\n",
    "resnet_rmse = np.sqrt(np.mean((dm_results_dict[args.loss_key]['dm_true'] - dm_results_dict[args.loss_key]['dm_pred'])**2))\n",
    "vit_rmse = np.sqrt(np.mean((y_test - vit_predictions)**2))\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('PERFORMANCE COMPARISON RESULTS')\n",
    "print('='*60)\n",
    "print(f'ResNet-FCN (Original GRAINet):')\n",
    "print(f'  Parameters: {resnet_params:,}')\n",
    "print(f'  MAE: {resnet_mae:.2f} cm')\n",
    "print(f'  RMSE: {resnet_rmse:.2f} cm')\n",
    "print(f'  Test samples: {len(dm_results_dict[args.loss_key][\"dm_true\"])}')\n",
    "\n",
    "print(f'\\nViT-Tiny (STRING2D-Cayley):')\n",
    "print(f'  Parameters: {vit_flax_params:,}')\n",
    "print(f'  MAE: {vit_mae:.2f} cm')\n",
    "print(f'  RMSE: {vit_rmse:.2f} cm')\n",
    "print(f'  Test samples: {len(y_test)}')\n",
    "print(f'  Training time: {training_time:.2f} seconds')\n",
    "\n",
    "print(f'\\nImprovement:')\n",
    "print(f'  MAE improvement: {((resnet_mae - vit_mae) / resnet_mae * 100):+.1f}%')\n",
    "print(f'  RMSE improvement: {((resnet_rmse - vit_rmse) / resnet_rmse * 100):+.1f}%')\n",
    "print(f'  Parameter efficiency: {(resnet_params / vit_flax_params):.1f}x fewer parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of Results\n",
    "\n",
    "Compare predictions from both models visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Plot comparison of predictions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# ResNet-FCN results\n",
    "mi, ma = 0, 20\n",
    "axes[0].scatter(dm_results_dict[args.loss_key]['dm_true'], dm_results_dict[args.loss_key]['dm_pred'], alpha=0.7)\n",
    "axes[0].set_xlabel('Ground truth mean diameter [cm]')\n",
    "axes[0].set_ylabel('Predicted mean diameter [cm]')\n",
    "axes[0].set_title(f'ResNet-FCN (MAE: {resnet_mae:.2f} cm)')\n",
    "axes[0].plot([mi, ma], [mi, ma], 'k--')\n",
    "axes[0].set_xlim(mi, ma)\n",
    "axes[0].set_ylim(mi, ma)\n",
    "axes[0].grid(True)\n",
    "axes[0].set_aspect('equal')\n",
    "\n",
    "# ViT-Tiny results\n",
    "axes[1].scatter(y_test, vit_predictions, alpha=0.7, color='orange')\n",
    "axes[1].set_xlabel('Ground truth mean diameter [cm]')\n",
    "axes[1].set_ylabel('Predicted mean diameter [cm]')\n",
    "axes[1].set_title(f'ViT-Tiny STRING2D-Cayley (MAE: {vit_mae:.2f} cm)')\n",
    "axes[1].plot([mi, ma], [mi, ma], 'k--')\n",
    "axes[1].set_xlim(mi, ma)\n",
    "axes[1].set_ylim(mi, ma)\n",
    "axes[1].grid(True)\n",
    "axes[1].set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot training curves for ViT\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "epochs = range(len(train_losses))\n",
    "ax.plot(epochs, train_losses, label='Training Loss', color='blue')\n",
    "ax.plot(epochs, test_losses, label='Test Loss', color='red')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Mean Squared Error')\n",
    "ax.set_title('ViT-Tiny Training Progress')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot sample predictions\n",
    "N_plots = min(8, len(X_test))\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(N_plots):\n",
    "    # Display original image (convert back to uint8)\n",
    "    img = (X_test[i] * 255).astype(np.uint8)\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].axis('off')\n",
    "    axes[i].set_title(f'True: {y_test[i]:.1f}\\nViT Pred: {vit_predictions[i]:.1f}')\n",
    "\n",
    "plt.suptitle('Sample Test Images with ViT-Tiny Predictions', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Conclusions\n",
    "\n",
    "Summary of the comparison between ResNet-FCN and ViT-Tiny with STRING2D-Cayley encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*80)\n",
    "print('GRAINET vs VIT-TINY COMPARISON SUMMARY')\n",
    "print('='*80)\n",
    "\n",
    "print('\\n🏗️  ARCHITECTURE COMPARISON:')\n",
    "print(f'  ResNet-FCN: Convolutional + Global Average Pooling')\n",
    "print(f'  ViT-Tiny: Transformer + STRING2D-Cayley Positional Encoding')\n",
    "\n",
    "print('\\n📊 PERFORMANCE METRICS:')\n",
    "improvement_mae = ((resnet_mae - vit_mae) / resnet_mae * 100)\n",
    "improvement_rmse = ((resnet_rmse - vit_rmse) / resnet_rmse * 100)\n",
    "print(f'  MAE improvement: {improvement_mae:+.1f}%')\n",
    "print(f'  RMSE improvement: {improvement_rmse:+.1f}%')\n",
    "\n",
    "print('\\n⚡ EFFICIENCY ANALYSIS:')\n",
    "param_ratio = resnet_params / vit_flax_params\n",
    "print(f'  ViT-Tiny uses {param_ratio:.1f}x fewer parameters')\n",
    "print(f'  Adaptive image preprocessing (scale + center crop)')\n",
    "print(f'  JAX JIT compilation for faster inference')\n",
    "\n",
    "print('\\n🔬 KEY INNOVATIONS:')\n",
    "print('  ✅ STRING2D-Cayley: Antisymmetric matrix + Cayley transform')\n",
    "print('  ✅ Learnable spatial relationships vs fixed CNN kernels')\n",
    "print('  ✅ Global attention mechanism')\n",
    "print('  ✅ Orthogonal transformations preserve geometric structure')\n",
    "print('  ✅ Adaptive to grain size distribution patterns')\n",
    "\n",
    "print('\\n🎯 NEXT STEPS:')\n",
    "print('  1. Full cross-validation evaluation (all 10 folds)')\n",
    "print('  2. Orthophoto prediction mapping comparison')\n",
    "print('  3. Multi-scale ViT implementation')\n",
    "print('  4. Hybrid CNN-ViT architecture')\n",
    "print('  5. Self-supervised pre-training on river imagery')\n",
    "\n",
    "status_icon = \"🎉\" if improvement_mae > 0 else \"⚠️\"\n",
    "print(f'\\n{status_icon} CONCLUSION:')\n",
    "if improvement_mae > 0:\n",
    "    print(f'  ViT-Tiny with STRING2D-Cayley shows {improvement_mae:.1f}% improvement over ResNet-FCN')\n",
    "    print('  Demonstrates effectiveness of learnable positional encoding for grain analysis')\n",
    "else:\n",
    "    print(f'  ViT-Tiny needs further tuning for optimal performance')\n",
    "    print('  Consider hyperparameter optimization and longer training')\n",
    "\n",
    "print('\\n📋 IMPLEMENTATION COMPLETE!')\n",
    "print('  Both models trained and evaluated on GRAINet demo data')\n",
    "print('  Ready for full dataset training and deployment')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}