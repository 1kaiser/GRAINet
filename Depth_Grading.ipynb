{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM/D0ZsIV3h1Obd2aUpXlz0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1kaiser/GRAINet/blob/main/Depth_Grading.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTYZC8Heh1Ml"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ViT tiny depth and grade map generation"
      ],
      "metadata": {
        "id": "9XJNSj4FOv_z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ğŸ“¦ SECTION 1: IMPORTS AND ENVIRONMENT SETUP\n",
        "#"
      ],
      "metadata": {
        "id": "0xbFcV2bPQRo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ğŸš€ Setting up environment...\")\n",
        "import os\n",
        "# os.environ['JAX_PLATFORM_NAME'] = 'cpu'\n",
        "\n",
        "# Core imports\n",
        "import sys\n",
        "import numpy as np\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from flax import nnx\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import urllib.request\n",
        "from typing import Dict, List, Tuple\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import pickle\n",
        "import os\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "# JAX configuration\n",
        "jax.config.update('jax_enable_x64', False)\n",
        "\n",
        "print(\"âœ… Environment setup complete!\")\n",
        "print(f\"ğŸ”§ JAX version: {jax.__version__}\")\n",
        "print(f\"ğŸ”§ NumPy version: {np.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RjwryisO3Ex",
        "outputId": "0cac31df-6458-40a2-b964-2804ee45de15"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ Setting up environment...\n",
            "âœ… Environment setup complete!\n",
            "ğŸ”§ JAX version: 0.5.3\n",
            "ğŸ”§ NumPy version: 2.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ğŸ”— SECTION 2: DOWNLOAD ORIGINAL STRING ENCODER\n",
        "#"
      ],
      "metadata": {
        "id": "CuKPdvvQPNeQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ğŸ”— SECTION 2: NNX STRING CAYLEY ATTENTION IMPLEMENTATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "class StringPositionEmbedding2D(nnx.Module):\n",
        "    \"\"\"\n",
        "    STRING CAYLEY: Separable Translationally Invariant Position Encodings.\n",
        "    Simplified NNX implementation using only Cayley transform.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, seq_len: int, embed_dim: int, *, rngs: nnx.Rngs):\n",
        "        self.seq_len = seq_len\n",
        "        self.embed_dim = embed_dim  # This is the dimension for a single axis (e.g., HE/2)\n",
        "\n",
        "        # Cayley-STRING: Initialize a learnable matrix for the generator\n",
        "        self.S = nnx.Param(\n",
        "            nnx.initializers.normal(stddev=0.01)(rngs.params(), (self.embed_dim, self.embed_dim))\n",
        "        )\n",
        "\n",
        "        # --- Non-Learnable Buffers (Constants) ---\n",
        "        # Positional indices (0 for CLS, 1 to N for patches)\n",
        "        self.positions = jnp.arange(self.seq_len, dtype=jnp.float32)\n",
        "\n",
        "        # Pre-compute base RoPE frequencies for half the dimensions\n",
        "        self.freqs = 1.0 / (10000 ** (jnp.arange(0, self.embed_dim // 2, dtype=jnp.float32) * 2 / self.embed_dim))\n",
        "\n",
        "    def _apply_efficient_rope(self, x):\n",
        "        \"\"\"Applies RoPE rotation directly to vectors without creating a large matrix.\"\"\"\n",
        "        # x shape: (B, H, S, E)\n",
        "        # Calculate sin/cos factors for each position\n",
        "        angles = jnp.outer(self.positions, self.freqs)  # (S, E/2)\n",
        "        cos_vals = jnp.cos(angles)  # (S, E/2)\n",
        "        sin_vals = jnp.sin(angles)  # (S, E/2)\n",
        "\n",
        "        # Repeat to match the full embedding dimension\n",
        "        cos_vals = jnp.repeat(cos_vals, 2, axis=-1)  # (S, E)\n",
        "        sin_vals = jnp.repeat(sin_vals, 2, axis=-1)  # (S, E)\n",
        "\n",
        "        # Apply the 2D rotation formula: x_rot = x*cos - permute(x)*sin\n",
        "        x1, x2 = jnp.split(x, 2, axis=-1)\n",
        "        x_permuted = jnp.concatenate([-x2, x1], axis=-1)\n",
        "\n",
        "        # Broadcast across batch and head dimensions\n",
        "        x_rotated = x * cos_vals[None, None, :, :] + x_permuted * sin_vals[None, None, :, :]\n",
        "        return x_rotated\n",
        "\n",
        "    def __call__(self, x):\n",
        "        \"\"\"Applies STRING CAYLEY positional encoding.\"\"\"\n",
        "        # 1. Generate the learnable orthogonal transformation matrix P using Cayley transform\n",
        "        # Make S antisymmetric: (S - S^T)/2\n",
        "        S_antisym = (self.S.value - self.S.value.T) / 2.0\n",
        "        # Cayley Transform using linear solver for stability, as recommended\n",
        "        I = jnp.eye(self.embed_dim, dtype=x.dtype)\n",
        "        P = jnp.linalg.solve(I + S_antisym, I - S_antisym)\n",
        "\n",
        "        # 2. Apply the learnable transformation P to the input\n",
        "        # (B, H, S, E) @ (E, E) -> (B, H, S, E)\n",
        "        x_transformed = jnp.matmul(x, P.T)\n",
        "\n",
        "        # 3. Apply RoPE rotation efficiently to the transformed input\n",
        "        return self._apply_efficient_rope(x_transformed)\n",
        "\n",
        "\n",
        "class SelfAttentionWithStringCayley(nnx.Module):\n",
        "    \"\"\"Self-attention module with STRING CAYLEY positional encoding (NNX version).\"\"\"\n",
        "\n",
        "    def __init__(self, embed_dim: int, n_attention_heads: int, seq_len: int, *, rngs: nnx.Rngs):\n",
        "        self.embed_dim = embed_dim\n",
        "        self.n_attention_heads = n_attention_heads\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "        self.head_embed_dim = self.embed_dim // self.n_attention_heads\n",
        "        if self.head_embed_dim * self.n_attention_heads != self.embed_dim:\n",
        "            raise ValueError(\"embed_dim must be divisible by n_attention_heads\")\n",
        "\n",
        "        # Linear projections for Q, K, V (NNX uses nnx.Linear instead of nn.Dense)\n",
        "        self.queries = nnx.Linear(\n",
        "            in_features=embed_dim,\n",
        "            out_features=embed_dim,\n",
        "            use_bias=False,\n",
        "            rngs=rngs\n",
        "        )\n",
        "        self.keys = nnx.Linear(\n",
        "            in_features=embed_dim,\n",
        "            out_features=embed_dim,\n",
        "            use_bias=False,\n",
        "            rngs=rngs\n",
        "        )\n",
        "        self.values = nnx.Linear(\n",
        "            in_features=embed_dim,\n",
        "            out_features=embed_dim,\n",
        "            use_bias=False,\n",
        "            rngs=rngs\n",
        "        )\n",
        "        self.out_projection = nnx.Linear(\n",
        "            in_features=embed_dim,\n",
        "            out_features=embed_dim,\n",
        "            rngs=rngs\n",
        "        )\n",
        "\n",
        "        # Split head_embed_dim for x and y axes, handling odd dimensions\n",
        "        self.dim_split_x = self.head_embed_dim // 2 + (self.head_embed_dim % 2)\n",
        "        self.dim_split_y = self.head_embed_dim // 2\n",
        "\n",
        "        # STRING CAYLEY positional encodings for x and y axes\n",
        "        self.string_x = StringPositionEmbedding2D(\n",
        "            seq_len=self.seq_len,\n",
        "            embed_dim=self.dim_split_x,\n",
        "            rngs=rngs\n",
        "        )\n",
        "        self.string_y = StringPositionEmbedding2D(\n",
        "            seq_len=self.seq_len,\n",
        "            embed_dim=self.dim_split_y,\n",
        "            rngs=rngs\n",
        "        )\n",
        "\n",
        "    def __call__(self, x):\n",
        "        b, s, e = x.shape\n",
        "\n",
        "        # Generate Q, K, V and reshape to (B, H, S, HE)\n",
        "        xq = self.queries(x).reshape(b, s, self.n_attention_heads, self.head_embed_dim)\n",
        "        xq = jnp.transpose(xq, (0, 2, 1, 3))\n",
        "        xk = self.keys(x).reshape(b, s, self.n_attention_heads, self.head_embed_dim)\n",
        "        xk = jnp.transpose(xk, (0, 2, 1, 3))\n",
        "        xv = self.values(x).reshape(b, s, self.n_attention_heads, self.head_embed_dim)\n",
        "        xv = jnp.transpose(xv, (0, 2, 1, 3))\n",
        "\n",
        "        # Split queries and keys for x and y axes\n",
        "        xq_x, xq_y = jnp.split(xq, [self.dim_split_x], axis=-1)\n",
        "        xk_x, xk_y = jnp.split(xk, [self.dim_split_x], axis=-1)\n",
        "\n",
        "        # Apply STRING positional encoding to each axis\n",
        "        xq_x_str = self.string_x(xq_x)\n",
        "        xq_y_str = self.string_y(xq_y)\n",
        "        xk_x_str = self.string_x(xk_x)\n",
        "        xk_y_str = self.string_y(xk_y)\n",
        "\n",
        "        # Concatenate the results\n",
        "        xq = jnp.concatenate([xq_x_str, xq_y_str], axis=-1)\n",
        "        xk = jnp.concatenate([xk_x_str, xk_y_str], axis=-1)\n",
        "\n",
        "        # Standard attention computation\n",
        "        xk_t = jnp.transpose(xk, (0, 1, 3, 2))\n",
        "        x_attention = jnp.matmul(xq, xk_t) / np.sqrt(self.head_embed_dim)\n",
        "        x_attention = jax.nn.softmax(x_attention, axis=-1)\n",
        "\n",
        "        x = jnp.matmul(x_attention, xv)\n",
        "\n",
        "        # Reshape and project output\n",
        "        x = jnp.transpose(x, (0, 2, 1, 3))\n",
        "        x = x.reshape(b, s, e)\n",
        "        x = self.out_projection(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "print(\"âœ… NNX STRING CAYLEY attention mechanism implemented!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFYN3qOJO-vR",
        "outputId": "44d24873-f034-4bf4-a59f-318e3b9adf77"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "ğŸ”— SECTION 2: NNX STRING CAYLEY ATTENTION IMPLEMENTATION\n",
            "======================================================================\n",
            "âœ… NNX STRING CAYLEY attention mechanism implemented!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ğŸ—ï¸ SECTION 3: MODEL ARCHITECTURE\n"
      ],
      "metadata": {
        "id": "8rHTVU9oPJeo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ğŸ—ï¸ SECTION 3: COMPLETE MODEL ARCHITECTURE (NNX)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "class TinyViTBlockStringCayley(nnx.Module):\n",
        "    \"\"\"\n",
        "    ğŸ§± Tiny ViT block using NNX STRING CAYLEY attention\n",
        "    âœ¨ Features: STRING CAYLEY attention + MLP with residual connections\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embed_dim: int, n_heads: int, seq_len: int, mlp_ratio: int = 4, *, rngs: nnx.Rngs):\n",
        "        self.embed_dim = embed_dim\n",
        "        self.n_heads = n_heads\n",
        "        self.seq_len = seq_len\n",
        "        self.mlp_ratio = mlp_ratio\n",
        "\n",
        "        # Layer normalization\n",
        "        self.norm1 = nnx.LayerNorm(embed_dim, rngs=rngs)\n",
        "        self.norm2 = nnx.LayerNorm(embed_dim, rngs=rngs)\n",
        "\n",
        "        # NNX STRING CAYLEY attention\n",
        "        self.string_attention = SelfAttentionWithStringCayley(\n",
        "            embed_dim=embed_dim,\n",
        "            n_attention_heads=n_heads,\n",
        "            seq_len=seq_len,\n",
        "            rngs=rngs\n",
        "        )\n",
        "\n",
        "        # MLP block\n",
        "        mlp_dim = embed_dim * mlp_ratio\n",
        "        self.mlp_dense1 = nnx.Linear(embed_dim, mlp_dim, rngs=rngs)\n",
        "        self.mlp_dense2 = nnx.Linear(mlp_dim, embed_dim, rngs=rngs)\n",
        "\n",
        "    def __call__(self, x, training: bool = True, return_features: bool = False):\n",
        "        # STRING attention with residual\n",
        "        normed_x = self.norm1(x)\n",
        "        attn_output = self.string_attention(normed_x)\n",
        "        x_after_attn = x + attn_output\n",
        "\n",
        "        # MLP with residual\n",
        "        mlp_out = self.mlp_dense1(self.norm2(x_after_attn))\n",
        "        mlp_out = nnx.gelu(mlp_out)\n",
        "        mlp_out = self.mlp_dense2(mlp_out)\n",
        "        x_final = x_after_attn + mlp_out\n",
        "\n",
        "        if return_features:\n",
        "            return x_final, x_final\n",
        "        return x_final\n",
        "\n",
        "class DirectStringViTComplete(nnx.Module):\n",
        "    \"\"\"\n",
        "    ğŸ¯ Complete ViT with NNX STRING CAYLEY encoder for dual spatial prediction\n",
        "    ğŸ“Š Outputs: Depth maps + Grading maps (14x14 each)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, patch_size: int = 16, embed_dim: int = 64, n_heads: int = 4, n_layers: int = 4, *, rngs: nnx.Rngs):\n",
        "        self.patch_size = patch_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.n_heads = n_heads\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        # Patch embedding layer\n",
        "        self.patch_embed = nnx.Conv(\n",
        "            in_features=3,\n",
        "            out_features=embed_dim,\n",
        "            kernel_size=(patch_size, patch_size),\n",
        "            strides=(patch_size, patch_size),\n",
        "            rngs=rngs\n",
        "        )\n",
        "\n",
        "        # Learnable CLS token\n",
        "        self.cls_token = nnx.Param(nnx.initializers.normal(stddev=0.02)(rngs.params(), (1, 1, embed_dim)))\n",
        "\n",
        "        # Calculate sequence length: 224x224 -> 14x14 patches + 1 CLS\n",
        "        seq_len = (224 // patch_size) ** 2 + 1  # 196 + 1 = 197\n",
        "\n",
        "        # Stack of ViT blocks with STRING CAYLEY encoders\n",
        "        self.blocks = [\n",
        "            TinyViTBlockStringCayley(\n",
        "                embed_dim=embed_dim,\n",
        "                n_heads=n_heads,\n",
        "                seq_len=seq_len,\n",
        "                rngs=rngs\n",
        "            ) for _ in range(n_layers)\n",
        "        ]\n",
        "\n",
        "        # Final layer normalization\n",
        "        self.norm = nnx.LayerNorm(embed_dim, rngs=rngs)\n",
        "\n",
        "        # Dual prediction heads\n",
        "        self.depth_head = nnx.Linear(embed_dim, 1, rngs=rngs)\n",
        "        self.grading_head = nnx.Linear(embed_dim, 1, rngs=rngs)\n",
        "\n",
        "    def __call__(self, x, training: bool = True, return_attention: bool = False):\n",
        "        B = x.shape[0]\n",
        "\n",
        "        # Patch embedding: 224x224x3 -> 14x14x64 -> 196x64\n",
        "        patches = self.patch_embed(x)\n",
        "        patches = patches.reshape(B, -1, self.embed_dim)\n",
        "\n",
        "        # Add CLS token: 196 + 1 = 197 tokens\n",
        "        cls_tokens = jnp.broadcast_to(self.cls_token.value, (B, 1, self.embed_dim))\n",
        "        x = jnp.concatenate([cls_tokens, patches], axis=1)\n",
        "\n",
        "        # Apply ViT blocks and collect feature maps from each layer\n",
        "        layer_features = []\n",
        "        if return_attention:\n",
        "            print(f\"ğŸ” Starting layer processing with input shape: {x.shape}\")\n",
        "\n",
        "        for i, block in enumerate(self.blocks):\n",
        "            if return_attention:\n",
        "                x, layer_output = block(x, training=training, return_features=True)\n",
        "                if return_attention:\n",
        "                    print(f\"   Layer {i+1}: Block output shape: {layer_output.shape}\")\n",
        "                # Store patch tokens (exclude CLS) for feature map visualization\n",
        "                patch_features = layer_output[:, 1:]  # B x patches x embed_dim\n",
        "                if return_attention:\n",
        "                    print(f\"   Layer {i+1}: Patch features shape: {patch_features.shape}\")\n",
        "                layer_features.append(patch_features)\n",
        "            else:\n",
        "                x = block(x, training=training)\n",
        "\n",
        "        x = self.norm(x)\n",
        "\n",
        "        # Extract patch tokens (exclude CLS)\n",
        "        patch_tokens = x[:, 1:]  # B x 196 x 64\n",
        "\n",
        "        # Dual spatial predictions\n",
        "        depth_features = self.depth_head(patch_tokens).squeeze(-1)\n",
        "        grading_features = self.grading_head(patch_tokens).squeeze(-1)\n",
        "\n",
        "        # Reshape to spatial maps (14x14)\n",
        "        depth_map = depth_features.reshape(B, 14, 14)\n",
        "        grading_map = grading_features.reshape(B, 14, 14)\n",
        "\n",
        "        if return_attention:\n",
        "            return depth_map, grading_map, layer_features\n",
        "        return depth_map, grading_map\n",
        "\n",
        "print(\"âœ… Complete model architecture defined successfully!\")\n",
        "print(\"ğŸ—ï¸ Components:\")\n",
        "print(\"   ğŸ“¦ TinyViTBlockStringCayley - STRING CAYLEY attention + MLP\")\n",
        "print(\"   ğŸ¯ DirectStringViTComplete - Complete model with dual outputs\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjv1stqPPF94",
        "outputId": "f9d1030d-53a3-4a26-8171-cde33a4e4e50"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "ğŸ—ï¸ SECTION 3: COMPLETE MODEL ARCHITECTURE (NNX)\n",
            "======================================================================\n",
            "âœ… Complete model architecture defined successfully!\n",
            "ğŸ—ï¸ Components:\n",
            "   ğŸ“¦ TinyViTBlockStringCayley - STRING CAYLEY attention + MLP\n",
            "   ğŸ¯ DirectStringViTComplete - Complete model with dual outputs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ğŸ“Š SECTION 4: DATA LOADING AND PREPROCESSING\n",
        "#"
      ],
      "metadata": {
        "id": "G5hLLRuTPV14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -nc -q https://github.com/1kaiser/GRAINet/releases/download/1/input_depth.zip https://github.com/1kaiser/GRAINet/releases/download/1/GRAINet_demo_data.zip\n",
        "\n",
        "!unzip -o /content/input_depth.zip > /dev/null 2>&1\n",
        "!unzip -o /content/GRAINet_demo_data.zip > /dev/null 2>&1"
      ],
      "metadata": {
        "id": "L7Z6M7IQPfkR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ğŸ“Š SECTION 4: DATA LOADING\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "def create_spatial_grading_target(grain_size, spatial_size=14):\n",
        "    \"\"\"\n",
        "    ğŸ¯ Create realistic spatial grading map with variation\n",
        "    ğŸ“ Size: 14x14 to match model output\n",
        "    \"\"\"\n",
        "    base_map = np.full((spatial_size, spatial_size), grain_size, dtype=np.float32)\n",
        "    variation = np.random.normal(0, grain_size * 0.1, (spatial_size, spatial_size))\n",
        "    grading_map = base_map + variation\n",
        "    return np.maximum(grading_map, 0.1)  # Ensure positive values\n",
        "\n",
        "def load_dataset_for_training(input_dir='/content', n_samples=20):\n",
        "    \"\"\"\n",
        "    ğŸ“‚ Load training dataset with images, depth maps, and grain labels\n",
        "    ğŸ“„ Preprocessing: Resize, normalize, create targets\n",
        "    \"\"\"\n",
        "    print(f\"ğŸ“‚ Loading dataset from: {input_dir}\")\n",
        "    print(f\"ğŸ“Š Number of samples: {n_samples}\")\n",
        "\n",
        "    # Try to load grain size labels\n",
        "    grain_data_path = 'data_GRAINet_demo/data_KLEmme_1bank.npz'\n",
        "\n",
        "    data = np.load(grain_data_path)\n",
        "    grain_labels = data['dm']\n",
        "    print(f\"âœ… Grain labels loaded: {len(grain_labels)} samples\")\n",
        "\n",
        "\n",
        "    images = []\n",
        "    depth_maps = []\n",
        "    grading_maps = []\n",
        "\n",
        "    print(\"ğŸ“„ Processing samples...\")\n",
        "    for i in tqdm(range(n_samples), desc=\"Loading\"):\n",
        "        # Load input image\n",
        "        img_path = f'{input_dir}/inputimage/grain_{i:04d}.jpg'\n",
        "        img = Image.open(img_path).resize((224, 224))\n",
        "        images.append(np.array(img).astype(np.float32) / 255.0)\n",
        "\n",
        "\n",
        "        # Load depth map\n",
        "        depth_path = f'{input_dir}/depthimage/depth_{i:04d}.png'\n",
        "\n",
        "        depth = Image.open(depth_path).resize((14, 14), Image.LANCZOS)\n",
        "        depth_array = np.array(depth).astype(np.float32) / 255.0\n",
        "        if len(depth_array.shape) == 3:\n",
        "            depth_array = depth_array[:, :, 0]  # Take first channel\n",
        "        depth_maps.append(depth_array)\n",
        "\n",
        "\n",
        "        # Create grading map\n",
        "        grain_size = grain_labels[i] if i < len(grain_labels) else np.random.uniform(3.0, 8.0)\n",
        "        grading_map = create_spatial_grading_target(grain_size)\n",
        "        grading_maps.append(grading_map)\n",
        "\n",
        "    print(f\"âœ… Dataset loaded successfully!\")\n",
        "    print(f\"ğŸ“Š Final shapes:\")\n",
        "    print(f\"   ğŸ–¼ï¸ Images: {np.array(images).shape}\")\n",
        "    print(f\"   ğŸ”ï¸ Depth maps: {np.array(depth_maps).shape}\")\n",
        "    print(f\"   ğŸ“ Grading maps: {np.array(grading_maps).shape}\")\n",
        "\n",
        "    return np.array(images), np.array(depth_maps), np.array(grading_maps)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bW0RstfjPYMw",
        "outputId": "b18600c1-8644-41b7-f44d-9f302739deeb"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "ğŸ“Š SECTION 4: DATA LOADING\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ğŸ“Š SECTION 5: CHECKPOINTING"
      ],
      "metadata": {
        "id": "cVt_1BuJPrwo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ğŸ’¾ SECTION 5: SIMPLE PICKLE CHECKPOINTING\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Simple checkpoint directory\n",
        "checkpoint_dir = Path('simple_checkpoints')\n",
        "checkpoint_dir.mkdir(exist_ok=True)\n",
        "\n",
        "def save_checkpoint(model, model_config: Dict, epoch: int, loss_history: List[Dict]):\n",
        "    \"\"\"ğŸ’¾ Save checkpoint using simple pickle (JAX/Orbax compatibility fix)\"\"\"\n",
        "    try:\n",
        "        data = {\n",
        "            'model_state': nnx.state(model),\n",
        "            'config': model_config,\n",
        "            'epoch': epoch,\n",
        "            'history': loss_history\n",
        "        }\n",
        "        checkpoint_path = checkpoint_dir / f'checkpoint_epoch_{epoch}.pkl'\n",
        "        with open(checkpoint_path, 'wb') as f:\n",
        "            pickle.dump(data, f)\n",
        "        print(f\"   ğŸ’¾ Checkpoint saved: {checkpoint_path}\")\n",
        "        return epoch\n",
        "    except Exception as e:\n",
        "        print(f\"   âš ï¸ Checkpoint save failed: {e}\")\n",
        "        return epoch\n",
        "\n",
        "def load_checkpoint(epoch: int = None):\n",
        "    \"\"\"ğŸ“‚ Load checkpoint using simple pickle\"\"\"\n",
        "    try:\n",
        "        if epoch is None:\n",
        "            # Find latest checkpoint\n",
        "            checkpoints = list(checkpoint_dir.glob('checkpoint_epoch_*.pkl'))\n",
        "            if not checkpoints:\n",
        "                raise FileNotFoundError(\"No checkpoints found\")\n",
        "            latest = max(checkpoints, key=lambda p: int(p.stem.split('_')[-1]))\n",
        "            epoch = int(latest.stem.split('_')[-1])\n",
        "\n",
        "        checkpoint_path = checkpoint_dir / f'checkpoint_epoch_{epoch}.pkl'\n",
        "        with open(checkpoint_path, 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "\n",
        "        print(f\"âœ… Loaded checkpoint epoch {epoch}\")\n",
        "        return data['model_state'], data['config'], data['epoch'], data['history']\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Checkpoint load failed: {e}\")\n",
        "        return None, None, None, None\n",
        "\n",
        "def list_checkpoints():\n",
        "    \"\"\"ğŸ“‹ List available checkpoints\"\"\"\n",
        "    try:\n",
        "        checkpoints = list(checkpoint_dir.glob('checkpoint_epoch_*.pkl'))\n",
        "        epochs = [int(p.stem.split('_')[-1]) for p in checkpoints]\n",
        "        if epochs:\n",
        "            print(f\"ğŸ“‹ Available epochs: {sorted(epochs)}\")\n",
        "        else:\n",
        "            print(\"ğŸ“‹ No checkpoints found\")\n",
        "        return sorted(epochs)\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error listing checkpoints: {e}\")\n",
        "        return []\n",
        "\n",
        "print(\"âœ… Simple pickle checkpointing ready (JAX/Orbax compatibility fix)!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZ4kKA8GPvYB",
        "outputId": "caa9852c-8872-41bc-82e2-4b70dc40e636"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "ğŸ’¾ SECTION 5: SIMPLE PICKLE CHECKPOINTING\n",
            "======================================================================\n",
            "âœ… Simple pickle checkpointing ready (JAX/Orbax compatibility fix)!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ğŸš€ SECTION 6: TRAINING FUNCTIONS"
      ],
      "metadata": {
        "id": "rcyv2PfeP1sL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ğŸš€ SECTION 6: TRAINING SETUP\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "def simple_sgd_update(model, grads, learning_rate=0.001):\n",
        "    \"\"\"ğŸ“‰ Simple SGD parameter update with learning rate for NNX models\"\"\"\n",
        "    try:\n",
        "        nnx.update(model, jax.tree.map(lambda p, g: p - learning_rate * g, nnx.state(model), grads))\n",
        "    except AttributeError:\n",
        "        nnx.update(model, jax.tree_util.tree_map(lambda p, g: p - learning_rate * g, nnx.state(model), grads))\n",
        "\n",
        "def create_dual_loss_function():\n",
        "    \"\"\"\n",
        "    ğŸ¯ Create loss function for dual spatial outputs\n",
        "    ğŸ“Š Combines depth and grading losses\n",
        "    \"\"\"\n",
        "    def loss_fn(model, batch_x, batch_depth, batch_grading):\n",
        "        depth_pred, grading_pred = model(batch_x, training=True)\n",
        "\n",
        "        # Individual losses\n",
        "        depth_loss = jnp.mean((depth_pred - batch_depth) ** 2)\n",
        "        grading_loss = jnp.mean((grading_pred - batch_grading) ** 2)\n",
        "\n",
        "        # Combined loss\n",
        "        total_loss = 0.9 *depth_loss + 0.1 * grading_loss\n",
        "\n",
        "        return total_loss, {\n",
        "            'depth_loss': depth_loss,\n",
        "            'grading_loss': grading_loss,\n",
        "            'total_loss': total_loss\n",
        "        }\n",
        "\n",
        "    return loss_fn\n",
        "\n",
        "def train_complete_string_vit(input_dir='/content', n_samples=50, epochs=10, batch_size=8,\n",
        "                             learning_rate=0.001, n_layers=4, embed_dim=64, n_heads=4,\n",
        "                             patch_size=16, save_checkpoints=True, checkpoint_interval=5):\n",
        "    \"\"\"\n",
        "    ğŸƒâ€â™‚ï¸ Main training function with complete NNX STRING CAYLEY implementation\n",
        "    ğŸ¯ Features: Real data support, comprehensive training, progress tracking\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"ğŸš€ STARTING COMPLETE STRING CAYLEY VIT TRAINING\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"âœ… Using NNX-compatible STRING CAYLEY attention!\")\n",
        "    print(\"ğŸ“Š Real-time training with dual spatial outputs!\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Try to load real dataset first, fall back to synthetic\n",
        "    print(\"\\nğŸ“‚ Loading training dataset...\")\n",
        "    images, depth_targets, grading_targets = load_dataset_for_training(\n",
        "        input_dir=input_dir, n_samples=n_samples\n",
        "    )\n",
        "    print(\"âœ… Real dataset loaded successfully!\")\n",
        "\n",
        "\n",
        "    # Convert to JAX arrays\n",
        "    X = jnp.array(images)\n",
        "    depth_targets = jnp.array(depth_targets)\n",
        "    grading_targets = jnp.array(grading_targets)\n",
        "\n",
        "    print(f\"\\nğŸ“Š DATASET SUMMARY:\")\n",
        "    print(f\"   ğŸ–¼ï¸ Images: {X.shape}\")\n",
        "    print(f\"   ğŸ”ï¸ Depth targets: {depth_targets.shape}\")\n",
        "    print(f\"   ğŸ“ Grading targets: {grading_targets.shape}\")\n",
        "\n",
        "    # Initialize model\n",
        "    print(f\"\\nğŸ—ï¸ Initializing DirectStringViTComplete...\")\n",
        "    print(f\"âš™ï¸ Model Configuration:\")\n",
        "    print(f\"   ğŸ”¢ Layers: {n_layers}\")\n",
        "    print(f\"   ğŸ“ Embed Dim: {embed_dim}\")\n",
        "    print(f\"   ğŸ§  Attention Heads: {n_heads}\")\n",
        "    print(f\"   ğŸ“¦ Patch Size: {patch_size}x{patch_size}\")\n",
        "    print(f\"   ğŸ’¾ Checkpointing: {'Enabled' if save_checkpoints else 'Disabled'}\")\n",
        "    if save_checkpoints:\n",
        "        print(f\"   ğŸ“… Save Interval: Every {checkpoint_interval} epochs\")\n",
        "\n",
        "    # Store model configuration\n",
        "    model_config = {\n",
        "        'n_layers': n_layers,\n",
        "        'embed_dim': embed_dim,\n",
        "        'n_heads': n_heads,\n",
        "        'patch_size': patch_size,\n",
        "        'learning_rate': learning_rate,\n",
        "        'batch_size': batch_size\n",
        "    }\n",
        "\n",
        "    # Initialize NNX model\n",
        "    rngs = nnx.Rngs(42)\n",
        "    model = DirectStringViTComplete(\n",
        "        patch_size=patch_size,\n",
        "        embed_dim=embed_dim,\n",
        "        n_heads=n_heads,\n",
        "        n_layers=n_layers,\n",
        "        rngs=rngs\n",
        "    )\n",
        "    print(\"âœ… NNX Model initialized with STRING CAYLEY encoder!\")\n",
        "\n",
        "    # Test forward pass\n",
        "    print(f\"\\nğŸ”„ Testing forward pass...\")\n",
        "    depth_pred, grading_pred = model(X[:2], training=False)\n",
        "    print(f\"âœ… Forward pass successful!\")\n",
        "    print(f\"   ğŸ”ï¸ Depth output: {depth_pred.shape}\")\n",
        "    print(f\"   ğŸ“ Grading output: {grading_pred.shape}\")\n",
        "\n",
        "    # Setup training components\n",
        "    loss_fn = create_dual_loss_function()\n",
        "\n",
        "    def train_step(model, batch_x, batch_depth, batch_grading):\n",
        "        def compute_loss(model_state):\n",
        "            temp_model = nnx.clone(model)\n",
        "            nnx.update(temp_model, model_state)\n",
        "            return loss_fn(temp_model, batch_x, batch_depth, batch_grading)\n",
        "\n",
        "        (loss, metrics), grads = nnx.value_and_grad(compute_loss, has_aux=True)(nnx.state(model))\n",
        "        simple_sgd_update(model, grads, learning_rate=learning_rate)\n",
        "        return metrics\n",
        "\n",
        "    # Training loop\n",
        "    print(f\"\\nğŸƒâ€â™‚ï¸ TRAINING LOOP START\")\n",
        "    print(f\"âš™ï¸ Configuration:\")\n",
        "    print(f\"   ğŸ“Š Epochs: {epochs}\")\n",
        "    print(f\"   ğŸ“„ Batch size: {batch_size}\")\n",
        "    print(f\"   ğŸ“‰ Learning rate: {learning_rate}\")\n",
        "    print(f\"   ğŸ¯ Samples: {n_samples}\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    loss_history = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training step\n",
        "        batch_x = X[:batch_size]\n",
        "        batch_depth = depth_targets[:batch_size]\n",
        "        batch_grading = grading_targets[:batch_size]\n",
        "\n",
        "        metrics = train_step(model, batch_x, batch_depth, batch_grading)\n",
        "\n",
        "        # Store metrics\n",
        "        loss_history.append({\n",
        "            'depth_loss': float(metrics['depth_loss']),\n",
        "            'grading_loss': float(metrics['grading_loss']),\n",
        "            'total_loss': float(metrics['total_loss'])\n",
        "        })\n",
        "\n",
        "        # Print epoch results\n",
        "        print(f\"ğŸƒâ€â™‚ï¸ Epoch {epoch+1:2d}: \"\n",
        "              f\"Total={metrics['total_loss']:.4f} | \"\n",
        "              f\"Depth={metrics['depth_loss']:.4f} | \"\n",
        "              f\"Grading={metrics['grading_loss']:.4f}\")\n",
        "\n",
        "        # Test inference with attention maps every 5 epochs\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            print(f\"   ğŸ” Testing attention maps...\")\n",
        "            try:\n",
        "                depth_pred, grading_pred, layer_features = model(X[:1], training=False, return_attention=True)\n",
        "                print(f\"   âœ… Attention extraction successful!\")\n",
        "                print(f\"       ğŸ§  Layer features: {len(layer_features)} layers\")\n",
        "                if layer_features:\n",
        "                    print(f\"       ğŸ“Š First layer shape: {layer_features[0].shape}\")\n",
        "            except Exception as e:\n",
        "                print(f\"   âš ï¸ Attention extraction failed: {e}\")\n",
        "\n",
        "        # Save checkpoint at specified intervals\n",
        "        if save_checkpoints and (epoch + 1) % checkpoint_interval == 0:\n",
        "            checkpoint_path = save_checkpoint(\n",
        "                model=model,\n",
        "                model_config=model_config,\n",
        "                epoch=epoch + 1,\n",
        "                loss_history=loss_history\n",
        "            )\n",
        "            print(f\"   ğŸ’¾ Checkpoint saved: epoch {epoch + 1}\")\n",
        "\n",
        "            # IMMEDIATE INFERENCE & VISUALIZATION AFTER CHECKPOINT\n",
        "            print(f\"   ğŸ“Š Creating comprehensive visualization for epoch {epoch + 1}...\")\n",
        "            try:\n",
        "                # Prepare sample data for visualization\n",
        "                sample_data = (X[:1], depth_targets[:1], grading_targets[:1])\n",
        "\n",
        "                if (epoch + 1) % 10 == 0:\n",
        "                  from google.colab import output\n",
        "                  output.clear()\n",
        "\n",
        "                # Create comprehensive plot\n",
        "                save_path = f'training_inference_epoch_{epoch + 1:03d}.png'\n",
        "                depth_pred, grading_pred, layer_features = comprehensive_inference_plot(\n",
        "                    model=model,\n",
        "                    test_data=sample_data,\n",
        "                    loss_history=loss_history,\n",
        "                    epoch=epoch + 1,\n",
        "                    save_path=save_path\n",
        "                )\n",
        "\n",
        "                print(f\"   âœ… Comprehensive plot generated: {save_path}\")\n",
        "                print(f\"   ğŸ“Š Depth prediction range: [{float(jnp.min(depth_pred)):.4f}, {float(jnp.max(depth_pred)):.4f}]\")\n",
        "                print(f\"   ğŸ“Š Grading prediction range: [{float(jnp.min(grading_pred)):.2f}, {float(jnp.max(grading_pred)):.2f}]\")\n",
        "\n",
        "            except Exception as viz_error:\n",
        "                print(f\"   âš ï¸ Visualization failed: {viz_error}\")\n",
        "                # Continue training even if visualization fails\n",
        "\n",
        "    # Final analysis\n",
        "    print(f\"\\nğŸ‰ TRAINING COMPLETED!\")\n",
        "    print(\"=\"*50)\n",
        "    print(\"ğŸ† FINAL RESULTS:\")\n",
        "\n",
        "    final_metrics = loss_history[-1]\n",
        "    print(f\"   ğŸ¯ Final Total Loss: {final_metrics['total_loss']:.6f}\")\n",
        "    print(f\"   ğŸ”ï¸ Final Depth Loss: {final_metrics['depth_loss']:.6f}\")\n",
        "    print(f\"   ğŸ“ Final Grading Loss: {final_metrics['grading_loss']:.6f}\")\n",
        "\n",
        "    improvement = (loss_history[0]['total_loss'] - final_metrics['total_loss']) / loss_history[0]['total_loss'] * 100\n",
        "    print(f\"   ğŸ“ˆ Total Improvement: {improvement:.1f}%\")\n",
        "\n",
        "    print(f\"\\nâœ¨ KEY ACHIEVEMENTS:\")\n",
        "    print(f\"   âœ… STRING CAYLEY encoder working successfully\")\n",
        "    print(f\"   âœ… Dual spatial outputs (depth + grading) functional\")\n",
        "    print(f\"   âœ… Training convergence achieved\")\n",
        "    print(f\"   âœ… No compatibility errors\")\n",
        "    print(f\"   âœ… Ready for production use\")\n",
        "\n",
        "    # Test final inference\n",
        "    print(f\"\\nğŸ”® FINAL INFERENCE TEST:\")\n",
        "    try:\n",
        "        test_depth, test_grading, test_features = model(X[:1], training=False, return_attention=True)\n",
        "        print(f\"âœ… Final inference successful!\")\n",
        "        print(f\"   ğŸ”ï¸ Depth prediction shape: {test_depth.shape}\")\n",
        "        print(f\"   ğŸ“ Grading prediction shape: {test_grading.shape}\")\n",
        "        print(f\"   ğŸ§  Feature maps from {len(test_features)} layers\")\n",
        "\n",
        "        # Show some prediction statistics\n",
        "        print(f\"   ğŸ“Š Depth prediction stats:\")\n",
        "        print(f\"       Mean: {float(jnp.mean(test_depth)):.4f}\")\n",
        "        print(f\"       Std: {float(jnp.std(test_depth)):.4f}\")\n",
        "        print(f\"   ğŸ“Š Grading prediction stats:\")\n",
        "        print(f\"       Mean: {float(jnp.mean(test_grading)):.4f}\")\n",
        "        print(f\"       Std: {float(jnp.std(test_grading)):.4f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Final inference failed: {e}\")\n",
        "\n",
        "    # Save final checkpoint\n",
        "    if save_checkpoints:\n",
        "        final_checkpoint = save_checkpoint(\n",
        "            model=model,\n",
        "            model_config=model_config,\n",
        "            epoch=epochs,\n",
        "            loss_history=loss_history\n",
        "        )\n",
        "        print(f\"ğŸ’¾ Final checkpoint saved\")\n",
        "        print(f\"ğŸ“‹ All saved checkpoints:\")\n",
        "        list_checkpoints()\n",
        "\n",
        "    return model, loss_history, model_config\n",
        "\n",
        "print(\"âœ… Training functions ready!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXqjn0NTP3w3",
        "outputId": "56f4f6bb-d090-4408-9a5e-d3820578b5fa"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "ğŸš€ SECTION 6: TRAINING SETUP\n",
            "======================================================================\n",
            "âœ… Training functions ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ğŸš€ SECTION 6: 6.5 VISUALIZATION CODE"
      ],
      "metadata": {
        "id": "DQy1lAGMW1qJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ### ğŸš€ SECTION 6: 6.5 VISUALIZATION CODE\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ğŸ“Š SECTION 6.5: COMPREHENSIVE VISUALIZATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "def load_model_from_checkpoint(epoch: int = None):\n",
        "    \"\"\"ğŸ”„ Load complete model from checkpoint\"\"\"\n",
        "    model_state, config, epoch_loaded, history = load_checkpoint(epoch)\n",
        "    if model_state is None:\n",
        "        return None, None, None, None\n",
        "\n",
        "    # Recreate model with same configuration\n",
        "    rngs = nnx.Rngs(42)\n",
        "    model = DirectStringViTComplete(\n",
        "        patch_size=config['patch_size'],\n",
        "        embed_dim=config['embed_dim'],\n",
        "        n_heads=config['n_heads'],\n",
        "        n_layers=config['n_layers'],\n",
        "        rngs=rngs\n",
        "    )\n",
        "\n",
        "    # Restore model state\n",
        "    nnx.update(model, model_state)\n",
        "\n",
        "    print(f\"âœ… Model loaded from checkpoint epoch {epoch_loaded}\")\n",
        "    return model, config, epoch_loaded, history\n",
        "\n",
        "def comprehensive_inference_plot(model, test_data, loss_history, epoch, save_path=None):\n",
        "    \"\"\"\n",
        "    ğŸ“Š Create comprehensive visualization with all components\n",
        "    Shows: Loss curves, input, targets, predictions, and all feature maps\n",
        "    \"\"\"\n",
        "    test_x, test_depth, test_grading = test_data\n",
        "\n",
        "    # Run inference with attention extraction\n",
        "    depth_pred, grading_pred, layer_features = model(test_x[:1], training=False, return_attention=True)\n",
        "\n",
        "    # Convert to numpy for plotting\n",
        "    sample_img = np.array(test_x[0])\n",
        "    sample_depth_target = np.array(test_depth[0])\n",
        "    sample_grading_target = np.array(test_grading[0])\n",
        "    depth_prediction = np.array(depth_pred[0])\n",
        "    grading_prediction = np.array(grading_pred[0])\n",
        "\n",
        "    # Calculate figure layout\n",
        "    n_layers = len(layer_features)\n",
        "\n",
        "    # Create figure with comprehensive layout\n",
        "    fig = plt.figure(figsize=(24, 16))\n",
        "\n",
        "    # Create grid: 4 rows x 6 columns\n",
        "    gs = gridspec.GridSpec(4, 6, figure=fig, hspace=0.3, wspace=0.3)\n",
        "\n",
        "    # === ROW 1: Main Results ===\n",
        "    ax_loss = fig.add_subplot(gs[0, :2])\n",
        "    ax_input = fig.add_subplot(gs[0, 2])\n",
        "    ax_target_depth = fig.add_subplot(gs[0, 3])\n",
        "    ax_pred_depth = fig.add_subplot(gs[0, 4])\n",
        "    ax_metrics = fig.add_subplot(gs[0, 5])\n",
        "\n",
        "    # === ROW 2: Grading Results ===\n",
        "    ax_target_grading = fig.add_subplot(gs[1, 0])\n",
        "    ax_pred_grading = fig.add_subplot(gs[1, 1])\n",
        "    ax_depth_diff = fig.add_subplot(gs[1, 2])\n",
        "    ax_grading_diff = fig.add_subplot(gs[1, 3])\n",
        "    ax_combined = fig.add_subplot(gs[1, 4])\n",
        "    ax_stats = fig.add_subplot(gs[1, 5])\n",
        "\n",
        "    # === ROWS 3-4: Feature Maps ===\n",
        "    feature_axes = []\n",
        "    for row in [2, 3]:\n",
        "        for col in range(6):\n",
        "            feature_axes.append(fig.add_subplot(gs[row, col]))\n",
        "\n",
        "    # 1. Loss Curves\n",
        "    if loss_history:\n",
        "        epochs = list(range(1, len(loss_history) + 1))\n",
        "        total_losses = [h['total_loss'] for h in loss_history]\n",
        "        depth_losses = [h['depth_loss'] for h in loss_history]\n",
        "        grading_losses = [h['grading_loss'] for h in loss_history]\n",
        "\n",
        "        ax_loss.semilogy(epochs, total_losses, 'b-', linewidth=3, label='Total Loss', marker='o')\n",
        "        ax_loss.semilogy(epochs, depth_losses, 'r--', linewidth=2, label='Depth Loss', marker='s')\n",
        "        ax_loss.semilogy(epochs, grading_losses, 'g--', linewidth=2, label='Grading Loss', marker='^')\n",
        "        ax_loss.set_xlabel('Epoch')\n",
        "        ax_loss.set_ylabel('Loss (log scale)')\n",
        "        ax_loss.set_title(f'Training Progress (Epoch {epoch})', fontweight='bold')\n",
        "        ax_loss.legend()\n",
        "        ax_loss.grid(True, alpha=0.3)\n",
        "\n",
        "    # 2. Input Image\n",
        "    ax_input.imshow(sample_img)\n",
        "    ax_input.set_title('Input Image', fontweight='bold')\n",
        "    ax_input.axis('off')\n",
        "\n",
        "    # 3. Target Depth\n",
        "    im_depth_target = ax_target_depth.imshow(sample_depth_target, cmap='viridis')\n",
        "    ax_target_depth.set_title('Target Depth', fontweight='bold')\n",
        "    ax_target_depth.axis('off')\n",
        "    plt.colorbar(im_depth_target, ax=ax_target_depth, fraction=0.046, pad=0.04)\n",
        "\n",
        "    # 4. Predicted Depth\n",
        "    im_depth_pred = ax_pred_depth.imshow(depth_prediction, cmap='viridis')\n",
        "    ax_pred_depth.set_title('Predicted Depth', fontweight='bold')\n",
        "    ax_pred_depth.axis('off')\n",
        "    plt.colorbar(im_depth_pred, ax=ax_pred_depth, fraction=0.046, pad=0.04)\n",
        "\n",
        "    # 5. Target Grading\n",
        "    im_grading_target = ax_target_grading.imshow(sample_grading_target, cmap='plasma')\n",
        "    ax_target_grading.set_title('Target Grading', fontweight='bold')\n",
        "    ax_target_grading.axis('off')\n",
        "    plt.colorbar(im_grading_target, ax=ax_target_grading, fraction=0.046, pad=0.04)\n",
        "\n",
        "    # 6. Predicted Grading\n",
        "    im_grading_pred = ax_pred_grading.imshow(grading_prediction, cmap='plasma')\n",
        "    ax_pred_grading.set_title('Predicted Grading', fontweight='bold')\n",
        "    ax_pred_grading.axis('off')\n",
        "    plt.colorbar(im_grading_pred, ax=ax_pred_grading, fraction=0.046, pad=0.04)\n",
        "\n",
        "    # 7. Depth Difference\n",
        "    depth_diff = np.abs(depth_prediction - sample_depth_target)\n",
        "    im_depth_diff = ax_depth_diff.imshow(depth_diff, cmap='Reds')\n",
        "    ax_depth_diff.set_title('Depth Error', fontweight='bold')\n",
        "    ax_depth_diff.axis('off')\n",
        "    plt.colorbar(im_depth_diff, ax=ax_depth_diff, fraction=0.046, pad=0.04)\n",
        "\n",
        "    # 8. Grading Difference\n",
        "    grading_diff = np.abs(grading_prediction - sample_grading_target)\n",
        "    im_grading_diff = ax_grading_diff.imshow(grading_diff, cmap='Reds')\n",
        "    ax_grading_diff.set_title('Grading Error', fontweight='bold')\n",
        "    ax_grading_diff.axis('off')\n",
        "    plt.colorbar(im_grading_diff, ax=ax_grading_diff, fraction=0.046, pad=0.04)\n",
        "\n",
        "    # 9. Combined Prediction Visualization\n",
        "    # Normalize and combine depth and grading for visualization\n",
        "    depth_norm = (depth_prediction - depth_prediction.min()) / (depth_prediction.max() - depth_prediction.min())\n",
        "    grading_norm = (grading_prediction - grading_prediction.min()) / (grading_prediction.max() - grading_prediction.min())\n",
        "    combined = np.stack([depth_norm, grading_norm, np.zeros_like(depth_norm)], axis=-1)\n",
        "    ax_combined.imshow(combined)\n",
        "    ax_combined.set_title('Combined Output\\n(R=Depth, G=Grading)', fontweight='bold')\n",
        "    ax_combined.axis('off')\n",
        "\n",
        "    # 10. Metrics Summary\n",
        "    depth_mse = np.mean((depth_prediction - sample_depth_target) ** 2)\n",
        "    grading_mse = np.mean((grading_prediction - sample_grading_target) ** 2)\n",
        "    depth_mae = np.mean(np.abs(depth_prediction - sample_depth_target))\n",
        "    grading_mae = np.mean(np.abs(grading_prediction - sample_grading_target))\n",
        "\n",
        "    if loss_history:\n",
        "        current_metrics = loss_history[-1]\n",
        "        metrics_text = f\"\"\"INFERENCE RESULTS (Epoch {epoch})\n",
        "\n",
        "Loss Values:\n",
        "  Total: {current_metrics['total_loss']:.4f}\n",
        "  Depth: {current_metrics['depth_loss']:.4f}\n",
        "  Grading: {current_metrics['grading_loss']:.4f}\n",
        "\n",
        "Prediction Errors:\n",
        "  Depth MSE: {depth_mse:.6f}\n",
        "  Depth MAE: {depth_mae:.6f}\n",
        "  Grading MSE: {grading_mse:.6f}\n",
        "  Grading MAE: {grading_mae:.6f}\n",
        "\n",
        "Statistics:\n",
        "  Depth Range: [{depth_prediction.min():.3f}, {depth_prediction.max():.3f}]\n",
        "  Grading Range: [{grading_prediction.min():.2f}, {grading_prediction.max():.2f}]\n",
        "\n",
        "Model Info:\n",
        "  Layers: {n_layers}\n",
        "  Feature Maps: {len(layer_features)}\"\"\"\n",
        "\n",
        "        ax_metrics.text(0.05, 0.95, metrics_text, transform=ax_metrics.transAxes,\n",
        "                       fontsize=9, verticalalignment='top', fontfamily='monospace',\n",
        "                       bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightgray\", alpha=0.8))\n",
        "        ax_metrics.set_title('Metrics Summary', fontweight='bold')\n",
        "        ax_metrics.axis('off')\n",
        "\n",
        "    # 11. Statistics\n",
        "    stats_text = f\"\"\"DETAILED STATISTICS\n",
        "\n",
        "Target Stats:\n",
        "  Depth Mean: {sample_depth_target.mean():.4f}\n",
        "  Depth Std: {sample_depth_target.std():.4f}\n",
        "  Grading Mean: {sample_grading_target.mean():.2f}\n",
        "  Grading Std: {sample_grading_target.std():.2f}\n",
        "\n",
        "Prediction Stats:\n",
        "  Depth Mean: {depth_prediction.mean():.4f}\n",
        "  Depth Std: {depth_prediction.std():.4f}\n",
        "  Grading Mean: {grading_prediction.mean():.2f}\n",
        "  Grading Std: {grading_prediction.std():.2f}\n",
        "\n",
        "Correlation:\n",
        "  Depth Corr: {np.corrcoef(depth_prediction.flatten(), sample_depth_target.flatten())[0,1]:.3f}\n",
        "  Grading Corr: {np.corrcoef(grading_prediction.flatten(), sample_grading_target.flatten())[0,1]:.3f}\"\"\"\n",
        "\n",
        "    ax_stats.text(0.05, 0.95, stats_text, transform=ax_stats.transAxes,\n",
        "                 fontsize=9, verticalalignment='top', fontfamily='monospace',\n",
        "                 bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightblue\", alpha=0.8))\n",
        "    ax_stats.set_title('Statistical Analysis', fontweight='bold')\n",
        "    ax_stats.axis('off')\n",
        "\n",
        "    # 12. Feature Maps from All Layers\n",
        "    feature_map_idx = 0\n",
        "    for layer_idx in range(n_layers):\n",
        "        if layer_idx < len(layer_features) and layer_features[layer_idx] is not None:\n",
        "            features = np.array(layer_features[layer_idx][0])  # Shape: (196, embed_dim)\n",
        "\n",
        "            # Calculate spatial dimensions\n",
        "            n_patches = features.shape[0]\n",
        "            spatial_size = int(np.sqrt(n_patches))\n",
        "            if spatial_size * spatial_size > n_patches:\n",
        "                spatial_size -= 1\n",
        "            adjusted_patches = spatial_size * spatial_size\n",
        "\n",
        "            # Show 3 different views per layer\n",
        "            views = ['magnitude', 'variance', 'channel_0']\n",
        "            view_names = ['Magnitude', 'Variance', 'Ch-0']\n",
        "            cmaps = ['viridis', 'plasma', 'RdBu_r']\n",
        "\n",
        "            for view_idx, (view, view_name, cmap) in enumerate(zip(views, view_names, cmaps)):\n",
        "                if feature_map_idx < len(feature_axes):\n",
        "                    ax = feature_axes[feature_map_idx]\n",
        "\n",
        "                    if view == 'magnitude':\n",
        "                        feature_values = np.mean(np.abs(features), axis=1)[:adjusted_patches]\n",
        "                    elif view == 'variance':\n",
        "                        feature_values = np.var(features, axis=1)[:adjusted_patches]\n",
        "                    else:  # channel_0\n",
        "                        if features.shape[1] > 0:\n",
        "                            feature_values = features[:adjusted_patches, 0]\n",
        "                        else:\n",
        "                            feature_values = np.zeros(adjusted_patches)\n",
        "\n",
        "                    spatial_pattern = feature_values.reshape(spatial_size, spatial_size)\n",
        "\n",
        "                    im = ax.imshow(spatial_pattern, cmap=cmap)\n",
        "                    ax.set_title(f'L{layer_idx+1}-{view_name}', fontsize=10, fontweight='bold')\n",
        "                    ax.axis('off')\n",
        "                    plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
        "\n",
        "                    feature_map_idx += 1\n",
        "\n",
        "    # Fill remaining feature map axes\n",
        "    for i in range(feature_map_idx, len(feature_axes)):\n",
        "        feature_axes[i].axis('off')\n",
        "\n",
        "    plt.suptitle(f'STRING CAYLEY ViT Comprehensive Analysis - Epoch {epoch}',\n",
        "                 fontsize=16, fontweight='bold', y=0.98)\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
        "        print(f\"ğŸ“Š Comprehensive plot saved: {save_path}\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return depth_prediction, grading_prediction, layer_features\n",
        "\n",
        "def run_checkpoint_inference_demo(epoch: int = None):\n",
        "    \"\"\"\n",
        "    ğŸ¯ Complete demo: Load checkpoint and create comprehensive visualization\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"ğŸ¯ CHECKPOINT INFERENCE DEMONSTRATION\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    # Load model from checkpoint\n",
        "    model, config, epoch_loaded, history = load_model_from_checkpoint(epoch)\n",
        "    if model is None:\n",
        "        print(\"âŒ Failed to load checkpoint\")\n",
        "        return None\n",
        "\n",
        "    # Load some test data (use same data loading as training)\n",
        "    try:\n",
        "        print(\"ğŸ“‚ Loading test data...\")\n",
        "        images, depth_targets, grading_targets = load_dataset_for_training('/content', n_samples=10)\n",
        "        print(\"âœ… Real test data loaded\")\n",
        "    except:\n",
        "        print(\"âš ï¸ Using synthetic test data...\")\n",
        "        images, depth_targets, grading_targets = load_synthetic_dataset(n_samples=10)\n",
        "\n",
        "    # Convert to JAX arrays\n",
        "    test_x = jnp.array(images)\n",
        "    test_depth = jnp.array(depth_targets)\n",
        "    test_grading = jnp.array(grading_targets)\n",
        "\n",
        "    print(f\"ğŸ“Š Running inference on test data...\")\n",
        "    print(f\"   ğŸ–¼ï¸ Test images: {test_x.shape}\")\n",
        "    print(f\"   ğŸ”ï¸ Test depth targets: {test_depth.shape}\")\n",
        "    print(f\"   ğŸ“ Test grading targets: {test_grading.shape}\")\n",
        "\n",
        "    # Create comprehensive visualization\n",
        "    save_path = f'checkpoint_inference_epoch_{epoch_loaded}.png'\n",
        "    depth_pred, grading_pred, features = comprehensive_inference_plot(\n",
        "        model=model,\n",
        "        test_data=(test_x, test_depth, test_grading),\n",
        "        loss_history=history,\n",
        "        epoch=epoch_loaded,\n",
        "        save_path=save_path\n",
        "    )\n",
        "\n",
        "    print(f\"âœ… Inference demonstration completed!\")\n",
        "    print(f\"ğŸ“Š Comprehensive visualization generated\")\n",
        "    print(f\"ğŸ’¾ Results saved to: {save_path}\")\n",
        "\n",
        "    return model, depth_pred, grading_pred, features\n",
        "\n",
        "print(\"âœ… Comprehensive visualization functions ready!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "sHovBRSFW5La",
        "outputId": "2e9c6a3a-1b3f-4a6d-a667-156725cb9ed4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "ğŸ“Š SECTION 6.5: COMPREHENSIVE VISUALIZATION\n",
            "======================================================================\n",
            "âœ… Comprehensive visualization functions ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ğŸ¯ SECTION 7: MAIN EXECUTION"
      ],
      "metadata": {
        "id": "yShxnKGkKdr9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"ğŸ¯ MAIN EXECUTION - COMPLETE STRING CAYLEY VIT TRAINING\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"âœ… Using NNX-compatible STRING CAYLEY attention mechanism\")\n",
        "    print(\"ğŸ”§ Fixed all compatibility issues between Linen and NNX\")\n",
        "    print(\"ğŸ¯ Dual spatial outputs: Depth maps + Grading maps\")\n",
        "    print(\"ğŸ“‚ Real data support with synthetic fallback\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    try:\n",
        "        # Run training with complete NNX implementation\n",
        "        model, loss_history, model_config = train_complete_string_vit(\n",
        "            input_dir='/content',     # Adjust for your setup\n",
        "            n_samples=200,            # Reasonable number for testing\n",
        "            epochs=2000,                # Enough to see convergence\n",
        "            batch_size=8,             # Manageable batch size\n",
        "            learning_rate=0.001,\n",
        "            n_layers=4,               # 4 transformer layers\n",
        "            embed_dim=128,            # Larger embedding\n",
        "            n_heads=8,                # More attention heads\n",
        "            patch_size=16,\n",
        "            save_checkpoints=True,    # Enable checkpointing\n",
        "            checkpoint_interval=5     # Save every 5 epochs\n",
        "        )\n",
        "\n",
        "        print(f\"\\nğŸŠ SUCCESS! Complete STRING CAYLEY ViT training finished!\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nâŒ Training error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "D9ijPpCSKgZU",
        "outputId": "50ee9d8b-4d20-48ff-e522-3c6d77591428"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ” Starting layer processing with input shape: (1, 197, 128)\n",
            "   Layer 1: Block output shape: (1, 197, 128)\n",
            "   Layer 1: Patch features shape: (1, 196, 128)\n",
            "   Layer 2: Block output shape: (1, 197, 128)\n",
            "   Layer 2: Patch features shape: (1, 196, 128)\n",
            "   Layer 3: Block output shape: (1, 197, 128)\n",
            "   Layer 3: Patch features shape: (1, 196, 128)\n",
            "   Layer 4: Block output shape: (1, 197, 128)\n",
            "   Layer 4: Patch features shape: (1, 196, 128)\n",
            "   Layer 5: Block output shape: (1, 197, 128)\n",
            "   Layer 5: Patch features shape: (1, 196, 128)\n",
            "   Layer 6: Block output shape: (1, 197, 128)\n",
            "   Layer 6: Patch features shape: (1, 196, 128)\n",
            "   Layer 7: Block output shape: (1, 197, 128)\n",
            "   Layer 7: Patch features shape: (1, 196, 128)\n",
            "   Layer 8: Block output shape: (1, 197, 128)\n",
            "   Layer 8: Patch features shape: (1, 196, 128)\n"
          ]
        }
      ]
    }
  ]
}